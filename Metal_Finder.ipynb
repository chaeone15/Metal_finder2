{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# @markdown # Step 1A: Install Conda in Colab\n",
        "# @markdown Please make sure to select a runtime with **High-RAM** before running this step\n",
        "!pip -q install -U condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "YPFCSbYcJOsI",
        "outputId": "2ad151df-cc83-45cf-9778-b42976828ae4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è¨ Downloading https://github.com/jaimergp/miniforge/releases/download/24.11.2-1_colab/Miniforge3-colab-24.11.2-1_colab-Linux-x86_64.sh...\n",
            "üì¶ Installing...\n",
            "üìå Adjusting configuration...\n",
            "ü©π Patching environment...\n",
            "‚è≤ Done in 0:00:15\n",
            "üîÅ Restarting kernel...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown # Step 1B: Match your environment to Colab‚Äôs Python (3.12)\n",
        "\n",
        "import os, sys\n",
        "\n",
        "# 1) Remove any existing pin file (it can force the wrong Python)\n",
        "pin_file = \"/usr/local/conda-meta/pinned\"\n",
        "if os.path.exists(pin_file):\n",
        "    os.remove(pin_file)\n",
        "\n",
        "# 2) Make Conda's base match the running kernel (3.12) and install deps\n",
        "!mamba install -y -c conda-forge \"python=3.12\" \"pymol-open-source=3.1.0=py312*\" biopython tqdm\n",
        "\n",
        "# 3) Quick smoke test\n",
        "print(\"Python:\", sys.version.split()[0])\n",
        "try:\n",
        "    import Bio, tqdm\n",
        "    print(\"Biopython:\", Bio.__version__, \"| tqdm:\", tqdm.__version__)\n",
        "except Exception as e:\n",
        "    print(\"Biopython/tqdm import error:\", e)\n",
        "\n",
        "try:\n",
        "    import pymol\n",
        "    pymol.finish_launching(['pymol','-cq'])\n",
        "    from pymol import cmd\n",
        "    print(\"PyMOL OK. Version:\", cmd.get_version())\n",
        "except Exception as e:\n",
        "    print(\"PyMOL import/launch error:\", e)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "IBXYqLhBHmVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown # Step 1C: Install all neccessary packages\n",
        "from Bio.PDB import PDBParser, Selection, NeighborSearch\n",
        "from Bio.PDB.Polypeptide import is_aa\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Optional: PyMOL headless (only if you need cmd)\n",
        "try:\n",
        "    import pymol\n",
        "    pymol.finish_launching(['pymol','-cq'])\n",
        "    from pymol import cmd\n",
        "except Exception as e:\n",
        "    print(\"PyMOL not available yet:\", e)\n",
        "\n",
        "# Optional: Torch\n",
        "try:\n",
        "    import torch\n",
        "except ImportError:\n",
        "    print(\"torch not found. Install with: pip install torch  (or: mamba install -y -c pytorch pytorch)\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "HGNke-geJZd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Step 2: Prepare PDB Files for Metal-Installer\n",
        "import os\n",
        "import glob\n",
        "import pymol2\n",
        "\n",
        "# === Configuration ===\n",
        "#@markdown ### Enter the path to the input directory:\n",
        "Input_pdb_directory = \"/content/\"  #@param {type:\"string\"}\n",
        "#@markdown ### Enter the path to the output directory:\n",
        "Target_pdb_directory = \"/content/\"  #@param {type:\"string\"}\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "os.makedirs(Target_pdb_directory, exist_ok=True)\n",
        "\n",
        "# Process all PDB files in the input directory\n",
        "pdb_files = glob.glob(os.path.join(Input_pdb_directory, \"*.pdb\"))\n",
        "\n",
        "for pdb_file in pdb_files:\n",
        "    pdb_filename = os.path.basename(pdb_file)\n",
        "    Output_file_path = os.path.join(Target_pdb_directory, pdb_filename)\n",
        "\n",
        "    with pymol2.PyMOL() as pymol:\n",
        "        pymol.cmd.reinitialize()\n",
        "        pymol.cmd.load(pdb_file)\n",
        "\n",
        "        # Remove water, ligands, and metals\n",
        "        pymol.cmd.remove(\"resn HOH\")  # Remove water\n",
        "        pymol.cmd.remove(\"hetatm\")    # Remove all heteroatoms (ligands, metals, etc.)\n",
        "\n",
        "        # Save cleaned PDB\n",
        "        pymol.cmd.save(Output_file_path)\n",
        "\n",
        "    print(f\"‚úÖ Processed: {pdb_filename}\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "nuukHJoFHl4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPrIazJuxjnp",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @markdown # Step 3A: Filter using geometric parameters (Containing all metal ligating residue)\n",
        "\n",
        "# pip install scipy\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from Bio.PDB import PDBParser\n",
        "import itertools\n",
        "import requests\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "import traceback\n",
        "from scipy.spatial import KDTree # <-- KDTree for fast neighbor search\n",
        "\n",
        "# --- Constants ---\n",
        "# Input directory containing PDB files to be processed\n",
        "Target_pdb_directory = \"/content/\" # @param {type:\"string\"}\n",
        "# Output directory where processed Excel files will be saved\n",
        "Result1_excel_directory = \"/content/\" # @param {type:\"string\"}\n",
        "# Create the output directory if it doesn't exist\n",
        "os.makedirs(Result1_excel_directory, exist_ok=True)\n",
        "\n",
        "# --- Download threshold configuration ---\n",
        "base_url = \"https://raw.githubusercontent.com/SNU-Songlab/Metal-Installer-code/main/Threshold\"\n",
        "Metal = 'Cu'  # @param [\"Zn\", \"Mn\", \"Cu\", \"Fe\"]\n",
        "Ligand = '2His_1Cys_only_for_Cu'  # @param [\"3His_only_for_Zn_Cu\", \"2His_1Asp_only_for_Zn_Mn_Fe\", \"2His_1Glu_only_for_Zn_Mn_Fe\", \"2His_1Cys_only_for_Cu\"]\n",
        "Threshold = '1'  # @param [\"1\", \"2\", \"3\", \"4\",\"5\"]\n",
        "thresholds_url = f\"{base_url}/{Metal}/{Ligand}/{Threshold}.xlsx\"\n",
        "thresholds_file = os.path.join(Result1_excel_directory, \"thresholds.xlsx\")\n",
        "\n",
        "print(f\"‚¨áÔ∏è Downloading thresholds from: {thresholds_url}\")\n",
        "response = requests.get(thresholds_url)\n",
        "if response.status_code == 200:\n",
        "    with open(thresholds_file, \"wb\") as file:\n",
        "        file.write(response.content)\n",
        "    print(f\"‚úÖ Thresholds downloaded successfully to {thresholds_file}\")\n",
        "else:\n",
        "    raise ValueError(f\"‚ùå Failed to download thresholds from {thresholds_url}. Status code: {response.status_code}\")\n",
        "\n",
        "# --- Load thresholds ---\n",
        "print(\"‚öôÔ∏è Loading thresholds...\")\n",
        "thresholds_df = pd.read_excel(thresholds_file, sheet_name=\"Sheet1\")\n",
        "thresholds = {\n",
        "    row[\"Parameter\"]: (row[\"Min\"], row[\"Max\"])\n",
        "    for _, row in thresholds_df.iterrows()\n",
        "    if pd.notna(row[\"Min\"]) and pd.notna(row[\"Max\"])\n",
        "}\n",
        "alpha_distance_range = thresholds[\"alpha_distance_range\"]\n",
        "beta_distance_range = thresholds[\"beta_distance_range\"]\n",
        "ratio_threshold_range = thresholds[\"ratio_threshold_range\"]\n",
        "pie_threshold_range = thresholds[\"pie_threshold_range\"]\n",
        "\n",
        "print(\"üìä Thresholds loaded:\")\n",
        "for key, value in thresholds.items():\n",
        "    print(f\"  - {key}: Min={value[0]}, Max={value[1]}\")\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def calculate_pie(v1, v2):\n",
        "    \"\"\"Calculates the angle (in degrees) between two vectors.\"\"\"\n",
        "    dot = np.dot(v1, v2)\n",
        "    norm = np.linalg.norm(v1) * np.linalg.norm(v2)\n",
        "    if norm == 0: return np.nan\n",
        "    angle_rad = np.arccos(np.clip(dot / norm, -1.0, 1.0))\n",
        "    return np.degrees(angle_rad)\n",
        "\n",
        "def standardize_residue_identity(row):\n",
        "    \"\"\"Creates a standardized, sorted tuple representing the triad's residues (by name and number).\"\"\"\n",
        "    res_names = [row[f\"Coord_residue_name{i+1}\"] for i in range(3)]\n",
        "    res_numbers = [row[f\"Coord_residue_number{i+1}\"] for i in range(3)]\n",
        "    items = list(zip(res_names, res_numbers))\n",
        "    items.sort()\n",
        "    return tuple(items)\n",
        "\n",
        "# --- Main Processing Function ---\n",
        "def process_pdb_file(pdb_file):\n",
        "    pdb_name = os.path.basename(pdb_file)\n",
        "    output_file = os.path.join(Result1_excel_directory, f\"{os.path.splitext(pdb_name)[0]}_processed.xlsx\")\n",
        "    print(f\"üîÑ Processing: {pdb_name}\")\n",
        "\n",
        "    try:\n",
        "        parser = PDBParser(QUIET=True)\n",
        "        structure = parser.get_structure(\"protein\", pdb_file)\n",
        "        model = structure[0]\n",
        "\n",
        "        if Metal == \"Cu\" and Ligand == \"2His_1Cys_only_for_Cu\":\n",
        "            target_residues = {\"HIS\", \"CYS\", \"ASP\", \"GLU\"}\n",
        "        else:\n",
        "            target_residues = {\"HIS\", \"ASP\", \"GLU\"}\n",
        "\n",
        "        all_residues_full = [res for chain in model for res in chain if res.get_id()[0] == \" \"]\n",
        "\n",
        "        residues_for_tree = [res for res in all_residues_full if res.has_id(\"CA\")]\n",
        "        if len(residues_for_tree) < 3:\n",
        "            print(f\"‚ö†Ô∏è Skipping {pdb_name}: Not enough residues with CA.\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        coords_ca = np.array([res[\"CA\"].coord for res in residues_for_tree])\n",
        "        residue_map = residues_for_tree\n",
        "        kdtree = KDTree(coords_ca)\n",
        "        max_dist = alpha_distance_range[1] * 1.1\n",
        "\n",
        "        pairs = kdtree.query_pairs(r=max_dist)\n",
        "\n",
        "        potential_triad_indices = set()\n",
        "        for i, j in pairs:\n",
        "            indices_k_near_i = kdtree.query_ball_point(coords_ca[i], r=max_dist)\n",
        "            indices_k_near_j = kdtree.query_ball_point(coords_ca[j], r=max_dist)\n",
        "            common_neighbors = set(indices_k_near_i).intersection(indices_k_near_j)\n",
        "            for k in common_neighbors:\n",
        "                if k != i and k != j:\n",
        "                    triad_indices = tuple(sorted((i, j, k)))\n",
        "                    potential_triad_indices.add(triad_indices)\n",
        "\n",
        "        triads_to_process = []\n",
        "        for idx_i, idx_j, idx_k in potential_triad_indices:\n",
        "            comb = (residue_map[idx_i], residue_map[idx_j], residue_map[idx_k])\n",
        "            if all(res.get_resname() in target_residues for res in comb):\n",
        "                triads_to_process.append(comb)\n",
        "\n",
        "        def get_triad_type(comb):\n",
        "            chains = [res.get_full_id()[2] for res in comb]\n",
        "            return \"intra\" if len(set(chains)) == 1 else \"inter\"\n",
        "\n",
        "        results = []\n",
        "        for comb in triads_to_process:\n",
        "            try:\n",
        "                if not all(res.has_id(\"CA\") and res.has_id(\"CB\") for res in comb):\n",
        "                    continue\n",
        "\n",
        "                alpha_distances, beta_distances = [], []\n",
        "                valid_distances = True\n",
        "                for res1, res2 in itertools.combinations(comb, 2):\n",
        "                    d_ca = np.linalg.norm(res1[\"CA\"].coord - res2[\"CA\"].coord)\n",
        "                    d_cb = np.linalg.norm(res1[\"CB\"].coord - res2[\"CB\"].coord)\n",
        "                    if not (alpha_distance_range[0] <= d_ca <= alpha_distance_range[1] and \\\n",
        "                            beta_distance_range[0] <= d_cb <= beta_distance_range[1]):\n",
        "                        valid_distances = False\n",
        "                        break\n",
        "                    alpha_distances.append(d_ca)\n",
        "                    beta_distances.append(d_cb)\n",
        "\n",
        "                if valid_distances and len(alpha_distances) == 3:\n",
        "                    row = {\n",
        "                        \"PDB_ID\": pdb_name,\n",
        "                        \"Triad_Type\": get_triad_type(comb),\n",
        "                    }\n",
        "                    for i, res in enumerate(comb):\n",
        "                        full_id = res.get_full_id()\n",
        "                        row[f\"Coord_chain_id_number{i+1}\"] = full_id[2]\n",
        "                        row[f\"Coord_residue_number{i+1}\"] = full_id[3][1]\n",
        "                        row[f\"Coord_residue_name{i+1}\"] = res.get_resname()\n",
        "                    for i in range(3):\n",
        "                        row[f\"Alpha Distance {i+1}\"] = alpha_distances[i]\n",
        "                        row[f\"Beta Distance {i+1}\"] = beta_distances[i]\n",
        "                    results.append(row)\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        df = pd.DataFrame(results)\n",
        "\n",
        "        if not df.empty:\n",
        "            def pass_ratio(row):\n",
        "                try:\n",
        "                    for i in range(3):\n",
        "                        if row[f\"Beta Distance {i+1}\"] == 0: return False\n",
        "                        ratio = row[f\"Alpha Distance {i+1}\"] / row[f\"Beta Distance {i+1}\"]\n",
        "                        if not (ratio_threshold_range[0] <= ratio <= ratio_threshold_range[1]):\n",
        "                            return False\n",
        "                    return True\n",
        "                except: return False\n",
        "            df_ratio = df[df.apply(pass_ratio, axis=1)].copy()\n",
        "        else:\n",
        "            df_ratio = pd.DataFrame()\n",
        "\n",
        "        if not df_ratio.empty:\n",
        "            res_lookup = {\n",
        "                (res.get_full_id()[2], res.get_full_id()[3][1]): res\n",
        "                for res in all_residues_full\n",
        "            }\n",
        "            def compute_pie(row):\n",
        "                try:\n",
        "                    comb_ids = [(row[f\"Coord_chain_id_number{i+1}\"], row[f\"Coord_residue_number{i+1}\"]) for i in range(3)]\n",
        "                    res_objs = [res_lookup[res_id] for res_id in comb_ids]\n",
        "                    angles = []\n",
        "                    for i, j in [(0,1), (0,2), (1,2)]:\n",
        "                        if not (res_objs[i].has_id(\"CA\") and res_objs[i].has_id(\"CB\") and \\\n",
        "                                res_objs[j].has_id(\"CA\") and res_objs[j].has_id(\"CB\")):\n",
        "                            return pd.Series([np.nan, np.nan, np.nan], index=[\"Pie_1_2\", \"Pie_1_3\", \"Pie_2_3\"])\n",
        "                        v_ca = res_objs[j][\"CA\"].coord - res_objs[i][\"CA\"].coord\n",
        "                        v_cb = res_objs[j][\"CB\"].coord - res_objs[i][\"CB\"].coord\n",
        "                        angles.append(calculate_pie(v_ca, v_cb))\n",
        "                    return pd.Series(angles, index=[\"Pie_1_2\", \"Pie_1_3\", \"Pie_2_3\"])\n",
        "                except:\n",
        "                    return pd.Series([np.nan, np.nan, np.nan], index=[\"Pie_1_2\", \"Pie_1_3\", \"Pie_2_3\"])\n",
        "\n",
        "            pie_results = df_ratio.apply(compute_pie, axis=1)\n",
        "            df_ratio[[\"Pie_1_2\", \"Pie_1_3\", \"Pie_2_3\"]] = pie_results\n",
        "            for col in [\"Pie_1_2\", \"Pie_1_3\", \"Pie_2_3\"]:\n",
        "                df_ratio[f\"{col}_Filter\"] = df_ratio[col].apply(\n",
        "                    lambda x: pie_threshold_range[0] < x < pie_threshold_range[1] if pd.notnull(x) else False)\n",
        "            df_ratio['Pie_Filter'] = df_ratio[[f'{col}_Filter' for col in ['Pie_1_2', 'Pie_1_3', 'Pie_2_3']]].all(axis=1)\n",
        "            df_final = df_ratio[df_ratio['Pie_Filter']].copy()\n",
        "        else:\n",
        "            df_final = pd.DataFrame()\n",
        "\n",
        "        if not df_final.empty:\n",
        "            df_final[\"Triad_Identity\"] = df_final.apply(standardize_residue_identity, axis=1)\n",
        "            df_deduplicated = df_final.drop_duplicates(subset=\"Triad_Identity\").drop(columns=[\"Triad_Identity\"])\n",
        "        else:\n",
        "            df_deduplicated = pd.DataFrame()\n",
        "\n",
        "        with pd.ExcelWriter(output_file) as writer:\n",
        "            df_deduplicated.to_excel(writer, sheet_name=\"4_Final_Deduplicated_KD\", index=False)\n",
        "        print(f\"‚úÖ Finished: {pdb_name} (Saved: {len(df_deduplicated)} triads)\")\n",
        "        return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error processing {pdb_name}: {e}\")\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "# --- Run Processing for All PDB Files ---\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n--- Starting Batch Processing with KDTree Pre-filtering ---\")\n",
        "    pdb_files = glob.glob(os.path.join(Target_pdb_directory, \"*.pdb\"))\n",
        "    print(f\"Found {len(pdb_files)} PDB files in {Target_pdb_directory}\")\n",
        "\n",
        "    if not pdb_files:\n",
        "        print(\"‚ö†Ô∏è No PDB files found. Exiting.\")\n",
        "    else:\n",
        "        num_workers = min(6, os.cpu_count() or 1)\n",
        "        print(f\"üöÄ Starting parallel processing with {num_workers} workers...\")\n",
        "\n",
        "        with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
        "            executor.map(process_pdb_file, pdb_files)\n",
        "\n",
        "    print(\"\\nüéâ All processing finished.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "OmyrAVvhfbQl"
      },
      "outputs": [],
      "source": [
        "# @markdown # Step 3B: Prepare coordinates file for density map analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from Bio.PDB import PDBParser\n",
        "\n",
        "Result1_excel_directory = \"/content/\" # @param {type:\"string\"}\n",
        "Target_pdb_directory = \"/content/cleaned\"    # @param {type:\"string\"}\n",
        "Coordinate_excel_directory = \"/content/coordinate\"  # @param {type:\"string\"}\n",
        "\n",
        "os.makedirs(Coordinate_excel_directory, exist_ok=True)\n",
        "\n",
        "# üß† Coordinate extraction helper\n",
        "def extract_coordinates(chain, res_id, atom_name):\n",
        "    try:\n",
        "        res_id = int(res_id)  # Ensure residue ID is integer\n",
        "        residue = chain[res_id]\n",
        "        return residue[atom_name].coord\n",
        "    except Exception:\n",
        "        return [None, None, None]\n",
        "\n",
        "# üîÅ Loop through all Excel files\n",
        "for file in os.listdir(Result1_excel_directory ):\n",
        "    if file.endswith(\"_processed.xlsx\"):\n",
        "        pdb_id = file.replace(\"_processed.xlsx\", \"\")\n",
        "        excel_path = os.path.join(Result1_excel_directory , file)\n",
        "        pdb_path = os.path.join(Target_pdb_directory, f\"{pdb_id}.pdb\")\n",
        "        output_path = os.path.join(Coordinate_excel_directory, f\"{pdb_id}_with_coordinates.xlsx\")\n",
        "\n",
        "        if not os.path.isfile(pdb_path):\n",
        "            print(f\"‚ùå Skipping {pdb_id}: PDB file not found.\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Load Excel\n",
        "            df_pie = pd.read_excel(excel_path, sheet_name=\"4_Final_Deduplicated_KD\")\n",
        "\n",
        "            # Load PDB\n",
        "            parser = PDBParser(QUIET=True)\n",
        "            structure = parser.get_structure(\"protein\", pdb_path)\n",
        "            chains = {chain.id: chain for chain in structure[0]}  # Assuming only one model\n",
        "\n",
        "            ca_coords, cb_coords = [], []\n",
        "\n",
        "            for idx, row in df_pie.iterrows():\n",
        "                chain1 = chains.get(row['Coord_chain_id_number1'])\n",
        "                chain2 = chains.get(row['Coord_chain_id_number2'])\n",
        "                chain3 = chains.get(row['Coord_chain_id_number3'])\n",
        "\n",
        "                # CŒ±\n",
        "                ca1 = extract_coordinates(chain1, row['Coord_residue_number1'], 'CA')\n",
        "                ca2 = extract_coordinates(chain2, row['Coord_residue_number2'], 'CA')\n",
        "                ca3 = extract_coordinates(chain3, row['Coord_residue_number3'], 'CA')\n",
        "                ca_coords.append([*ca1, *ca2, *ca3])\n",
        "\n",
        "                # CŒ≤\n",
        "                cb1 = extract_coordinates(chain1, row['Coord_residue_number1'], 'CB')\n",
        "                cb2 = extract_coordinates(chain2, row['Coord_residue_number2'], 'CB')\n",
        "                cb3 = extract_coordinates(chain3, row['Coord_residue_number3'], 'CB')\n",
        "                cb_coords.append([*cb1, *cb2, *cb3])\n",
        "\n",
        "            # Convert to DataFrame\n",
        "            ca_cols = ['CA1_X', 'CA1_Y', 'CA1_Z', 'CA2_X', 'CA2_Y', 'CA2_Z', 'CA3_X', 'CA3_Y', 'CA3_Z']\n",
        "            cb_cols = ['CB1_X', 'CB1_Y', 'CB1_Z', 'CB2_X', 'CB2_Y', 'CB2_Z', 'CB3_X', 'CB3_Y', 'CB3_Z']\n",
        "            df_ca = pd.DataFrame(ca_coords, columns=ca_cols)\n",
        "            df_cb = pd.DataFrame(cb_coords, columns=cb_cols)\n",
        "\n",
        "            # Combine and save\n",
        "            df_pie = pd.concat([df_pie.reset_index(drop=True), df_ca, df_cb], axis=1)\n",
        "            if 'PDB_ID' in df_pie.columns:\n",
        "                df_pie['PDB_ID'] = df_pie['PDB_ID'].str.replace('.pdb', '', regex=False)\n",
        "\n",
        "            df_pie.to_excel(output_path, index=False)\n",
        "            print(f\"‚úÖ Saved: {output_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error processing {pdb_id}: {e}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3FRNkvrAmVqw"
      },
      "outputs": [],
      "source": [
        "# @markdown # Step 4: Filter using probability density map analysis\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "import requests\n",
        "import traceback # Import traceback for better error printing\n",
        "import multiprocessing # Import multiprocessing\n",
        "from Bio.PDB import PDBParser # PDBParser is still needed for structure loading\n",
        "from scipy.spatial import KDTree # Import KDTree\n",
        "\n",
        "# --- Configuration and Setup ---\n",
        "# Define base directories (MODIFY THESE PATHS IF NEEDED)\n",
        "# Using the paths from your last provided code\n",
        "Coordinate_excel_directory = \"/content/coordinate\" # @param {type:\"string\"}\n",
        "Target_pdb_directory = \"/content/cleaned\"                    # @param {type:\"string\"}\n",
        "Result_2_excel_directory = \"/content/result/\" # @param {type:\"string\"}\n",
        "\n",
        "# Check if base directories exist\n",
        "if not os.path.exists(Coordinate_excel_directory):\n",
        "    raise FileNotFoundError(f\"Excel directory not found: {Coordinate_excel_directory}\")\n",
        "if not os.path.exists(Target_pdb_directory):\n",
        "    raise FileNotFoundError(f\"PDB directory not found: {Target_pdb_directory}\")\n",
        "os.makedirs(Result_2_excel_directory, exist_ok=True)\n",
        "\n",
        "# Define file paths for downloaded data\n",
        "prob_map_file = '/content/map.xlsx'\n",
        "thresholds_file = '/content/threshold.xlsx'\n",
        "\n",
        "# --- Download Data from GitHub ---\n",
        "# Using the parameters from your last provided code\n",
        "base_url = \"https://raw.githubusercontent.com/SNU-Songlab/Metal-Installer-code/main/probability/\"\n",
        "Metal = 'Zn'  # @param [\"Zn\", \"Mn\", \"Cu\", \"Fe\"]\n",
        "Ligand = '2His_1Glu_only_for_Zn_Mn_Fe'  # @param [\"3His_only_for_Zn_Cu\", \"2His_1Asp_only_for_Zn_Mn_Fe\", \"2His_1Glu_only_for_Zn_Mn_Fe\", \"2His_1Cys_only_for_Cu\"]\n",
        "map_url = f\"{base_url}/{Metal}/{Ligand }/map.xlsx\"\n",
        "thresholds_url = f\"{base_url}/{Metal}/{Ligand }/threshold.xlsx\"\n",
        "\n",
        "print(f\"Downloading probability map from: {map_url}\")\n",
        "response_map = requests.get(map_url)\n",
        "if response_map.status_code == 200:\n",
        "    with open(prob_map_file, 'wb') as file:\n",
        "        file.write(response_map.content)\n",
        "    print(f\"Downloaded map data to {prob_map_file}\")\n",
        "else:\n",
        "    raise ValueError(f\"Failed to download map file from {map_url}. Status code: {response_map.status_code}\")\n",
        "\n",
        "print(f\"Downloading thresholds from: {thresholds_url}\")\n",
        "response_thresh = requests.get(thresholds_url)\n",
        "if response_thresh.status_code == 200:\n",
        "    with open(thresholds_file, 'wb') as file:\n",
        "        file.write(response_thresh.content)\n",
        "    print(f\"Downloaded thresholds data to {thresholds_file}\")\n",
        "else:\n",
        "    raise ValueError(f\"Failed to download thresholds file from {thresholds_url}. Status code: {response_thresh.status_code}\")\n",
        "\n",
        "# --- Load and Process Data --- (Load data ONCE in the main process)\n",
        "print(\"Loading thresholds...\")\n",
        "thresholds_df = pd.read_excel(thresholds_file, sheet_name='Sheet1')\n",
        "print(\"Loading probability map...\")\n",
        "df_precomputed_prob_map = pd.read_excel(prob_map_file)\n",
        "\n",
        "print(\"Processing thresholds...\")\n",
        "thresholds = {}\n",
        "for _, row in thresholds_df.iterrows():\n",
        "    parameter = row['Parameter']\n",
        "    min_value = row['Min']\n",
        "    max_value = row['Max']\n",
        "    if pd.notna(min_value) and pd.notna(max_value):\n",
        "        thresholds[parameter] = (min_value, max_value)\n",
        "\n",
        "required_keys = ['ca_distances_calc', 'cb_distances_calc', 'ratio', 'angle']\n",
        "for key in required_keys:\n",
        "    if key not in thresholds:\n",
        "        raise KeyError(f\"Missing key '{key}' in thresholds file.\")\n",
        "\n",
        "print(\"Processing probability map...\")\n",
        "ca_bins = np.sort(df_precomputed_prob_map['Calpha_Zn_Dist'].unique())\n",
        "cb_bins = np.sort(df_precomputed_prob_map['Cbeta_Zn_Dist'].unique())\n",
        "angle_bins = np.sort(df_precomputed_prob_map['CA-Zn-CB_Angle'].unique())\n",
        "pivoted_prob_map = df_precomputed_prob_map.pivot_table(\n",
        "    index='Calpha_Zn_Dist', columns=['Cbeta_Zn_Dist', 'CA-Zn-CB_Angle'], values='Probability', fill_value=0\n",
        ")\n",
        "expected_shape = (len(ca_bins), len(cb_bins) * len(angle_bins))\n",
        "if pivoted_prob_map.shape == expected_shape:\n",
        "     prob_map_3d = pivoted_prob_map.values.reshape((len(ca_bins), len(cb_bins), len(angle_bins)))\n",
        "     print(\"Probability map processed into 3D array.\")\n",
        "else:\n",
        "     raise ValueError(f\"Pivoted map shape {pivoted_prob_map.shape} doesn't match expected shape {expected_shape} for reshaping.\")\n",
        "\n",
        "\n",
        "# --- Helper Function Definitions ---\n",
        "\n",
        "def calculate_ratio(current_point, ca_xyz, cb_xyz):\n",
        "    # ... (no changes needed) ...\n",
        "    ca_distances = np.linalg.norm(ca_xyz - current_point, axis=1)\n",
        "    cb_distances = np.linalg.norm(cb_xyz - current_point, axis=1)\n",
        "    ratios = np.divide(ca_distances, cb_distances, out=np.full_like(ca_distances, np.inf), where=cb_distances!=0)\n",
        "    return ratios\n",
        "\n",
        "def load_pdb_structure(entry_id, pdb_directory):\n",
        "    # ... (no changes needed) ...\n",
        "    pdb_parser = PDBParser(QUIET=True)\n",
        "    pdb_file_path = os.path.join(pdb_directory, f\"{entry_id}.pdb\")\n",
        "    try:\n",
        "        structure = pdb_parser.get_structure(entry_id, pdb_file_path)\n",
        "        return structure\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå PDB file not found for loading: {pdb_file_path}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading PDB file {pdb_file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def score_zn_predictions(ca_distances, cb_distances, angles, prob_map_3d, ca_bins, cb_bins, angle_bins):\n",
        "    # ... (no changes needed) ...\n",
        "    ca_bin_indices = np.clip(np.digitize(ca_distances, ca_bins[1:], right=True), 0, len(ca_bins)-1)\n",
        "    cb_bin_indices = np.clip(np.digitize(cb_distances, cb_bins[1:], right=True), 0, len(cb_bins)-1)\n",
        "    angle_bin_indices = np.clip(np.digitize(angles, angle_bins[1:], right=True), 0, len(angle_bins)-1)\n",
        "    probabilities = []\n",
        "    valid = True\n",
        "    for cbin, bbin, abin in zip(ca_bin_indices, cb_bin_indices, angle_bin_indices):\n",
        "        if 0 <= cbin < prob_map_3d.shape[0] and 0 <= bbin < prob_map_3d.shape[1] and 0 <= abin < prob_map_3d.shape[2]:\n",
        "            prob_value = prob_map_3d[cbin, bbin, abin]\n",
        "            if prob_value <= 0:\n",
        "                valid = False\n",
        "                break\n",
        "            probabilities.append(prob_value)\n",
        "        else:\n",
        "            # print(f\"‚ö†Ô∏è Warning: Invalid bin indices generated: CA({cbin}), CB({bbin}), Angle({abin})\")\n",
        "            valid = False\n",
        "            break\n",
        "    final_score = np.prod(probabilities) if valid and probabilities else 0.0\n",
        "    return final_score\n",
        "\n",
        "def calculate_angles(zn_coords, ca_coords_triplet, cb_coords_triplet):\n",
        "    # ... (no changes needed) ...\n",
        "    angles = []\n",
        "    for i in range(3):\n",
        "        v_ca = ca_coords_triplet[i] - zn_coords\n",
        "        v_cb = cb_coords_triplet[i] - zn_coords\n",
        "        norm_v_ca = np.linalg.norm(v_ca)\n",
        "        norm_v_cb = np.linalg.norm(v_cb)\n",
        "        if norm_v_ca == 0 or norm_v_cb == 0:\n",
        "            angles.append(0.0)\n",
        "            continue\n",
        "        cos_theta = np.dot(v_ca, v_cb) / (norm_v_ca * norm_v_cb)\n",
        "        angle_rad = np.arccos(np.clip(cos_theta, -1.0, 1.0))\n",
        "        angles.append(np.degrees(angle_rad))\n",
        "    return angles\n",
        "\n",
        "def define_excluded_triads(triad_res_nums, structure):\n",
        "    # ... (no changes needed) ...\n",
        "    excluded_residues = set()\n",
        "    if structure is None: return excluded_residues\n",
        "    # Ensure triad_res_nums are integers for comparison\n",
        "    try:\n",
        "        res_nums_to_find = set(int(num) for num in triad_res_nums)\n",
        "    except (ValueError, TypeError):\n",
        "         print(f\"‚ö†Ô∏è Warning: Could not convert all triad residue numbers {triad_res_nums} to integers.\")\n",
        "         return excluded_residues # Return empty set if conversion fails\n",
        "\n",
        "    for model in structure:\n",
        "        for chain in model:\n",
        "            for residue in chain:\n",
        "                res_seq_num = residue.id[1]\n",
        "                if res_seq_num in res_nums_to_find:\n",
        "                    excluded_residues.add((chain.id, res_seq_num))\n",
        "    return excluded_residues\n",
        "\n",
        "# --- NEW Proximity Filter using KDTree ---\n",
        "def proximity_filter_kdtree(kdtree, zn_candidate, exclusion_radius=2.5):\n",
        "    \"\"\"\n",
        "    Checks proximity using a pre-built SciPy KDTree.\n",
        "    Returns True if valid (no atoms too close), False otherwise.\n",
        "    \"\"\"\n",
        "    if kdtree is None:\n",
        "        # If no tree was built (e.g., no non-excluded atoms), assume valid\n",
        "        return True\n",
        "\n",
        "    try:\n",
        "        # Query the KDTree to find indices of points within the radius\n",
        "        # query_ball_point is efficient for this \"are there any?\" check\n",
        "        indices_nearby = kdtree.query_ball_point(zn_candidate, r=exclusion_radius, return_length=True)\n",
        "\n",
        "        # If the length is > 0, points were found nearby\n",
        "        if indices_nearby > 0:\n",
        "            return False # Invalid: atoms are too close\n",
        "        else:\n",
        "            return True # Valid: no atoms found within the radius\n",
        "    except Exception as e:\n",
        "        # Handle potential errors during KDTree query phase\n",
        "        print(f\"‚ùå Error during KDTree query: {e}\")\n",
        "        return False # Treat query errors as failing the proximity check\n",
        "\n",
        "\n",
        "# --- MODIFIED Main Prediction Function (Uses KDTree) ---\n",
        "def estimate_zn_iterative(\n",
        "    ca_coords_site_flat, # Coords for ONE site - FLAT array (9,) expected\n",
        "    cb_coords_site_flat, # Coords for ONE site - FLAT array (9,) expected\n",
        "    site_info,      # DataFrame row or dict with PDB_ID and residue numbers\n",
        "    structure,      # Pass the loaded structure\n",
        "    thresholds,     # Pass the thresholds dict\n",
        "    prob_map_3d, ca_bins, cb_bins, angle_bins, # Pass map and bins\n",
        "    grid_resolution=0.2\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Estimates Zn coordinate for a SINGLE site using KDTree for proximity,\n",
        "    returning the FIRST valid candidate.\n",
        "    \"\"\"\n",
        "    entry_id = site_info['PDB_ID']\n",
        "    site_index_name = site_info.name\n",
        "\n",
        "    # --- Coordinate Validation and Reshape --- (Includes fix from before)\n",
        "    try:\n",
        "        ca_coords_numeric = pd.to_numeric(np.asarray(ca_coords_site_flat), errors='coerce')\n",
        "        cb_coords_numeric = pd.to_numeric(np.asarray(cb_coords_site_flat), errors='coerce')\n",
        "        if np.isnan(ca_coords_numeric).any() or np.isnan(cb_coords_numeric).any():\n",
        "             return \"no metal\", 0, [None, None, None]\n",
        "        if ca_coords_numeric.shape != (9,) or cb_coords_numeric.shape != (9,):\n",
        "             return \"no metal\", 0, [None, None, None]\n",
        "        ca_xyz = ca_coords_numeric.astype(np.float64).reshape(3, 3)\n",
        "        cb_xyz = cb_coords_numeric.astype(np.float64).reshape(3, 3)\n",
        "    except (ValueError, TypeError) as e:\n",
        "        print(f\"‚ùå Error validating/reshaping coordinates for site index {site_index_name} in {entry_id}: {e}\")\n",
        "        return \"no metal\", 0, [None, None, None]\n",
        "\n",
        "    if structure is None: return \"no metal\", 0, [None, None, None]\n",
        "\n",
        "    # --- Define Excluded Residues ---\n",
        "    triad_res_nums = [\n",
        "        site_info['Coord_residue_number1'],\n",
        "        site_info['Coord_residue_number2'],\n",
        "        site_info['Coord_residue_number3']\n",
        "    ]\n",
        "    excluded_residues = define_excluded_triads(triad_res_nums, structure) # Get set of (chain, resnum)\n",
        "\n",
        "    # --- Build KD-Tree for Proximity Check (for this specific site's excluded residues) ---\n",
        "    non_excluded_coords_list = []\n",
        "    for atom in structure.get_atoms():\n",
        "        residue = atom.get_parent()\n",
        "        chain = residue.get_parent()\n",
        "        res_info = (chain.id, residue.id[1])\n",
        "        if res_info not in excluded_residues:\n",
        "            # Optional: Skip Hydrogens if needed\n",
        "            # if atom.element == 'H': continue\n",
        "            non_excluded_coords_list.append(atom.coord)\n",
        "\n",
        "    kdtree = None # Initialize kdtree\n",
        "    if non_excluded_coords_list:\n",
        "        try:\n",
        "             non_excluded_coords = np.array(non_excluded_coords_list, dtype=np.float64)\n",
        "             # Check if array is valid before building tree\n",
        "             if non_excluded_coords.ndim == 2 and non_excluded_coords.shape[1] == 3:\n",
        "                  kdtree = KDTree(non_excluded_coords)\n",
        "             # else: print(f\"‚ö†Ô∏è Warning: Invalid shape {non_excluded_coords.shape} for KDTree points in {entry_id}, site {site_index_name}\") # Less verbose\n",
        "        except Exception as kdtree_error:\n",
        "             print(f\"‚ùå Error building KDTree for {entry_id}, site {site_index_name}: {kdtree_error}\")\n",
        "             # kdtree remains None, proximity_filter_kdtree will handle this\n",
        "\n",
        "    # --- Define Search Space (Outer Box Intersection) ---\n",
        "    # ... (calculation remains the same) ...\n",
        "    shared_x_min, shared_x_max = -np.inf, np.inf\n",
        "    shared_y_min, shared_y_max = -np.inf, np.inf\n",
        "    shared_z_min, shared_z_max = -np.inf, np.inf\n",
        "    for j in range(3):\n",
        "        x_min_outer = min(ca_xyz[j, 0], cb_xyz[j, 0]) - thresholds['ca_distances_calc'][1]\n",
        "        x_max_outer = max(ca_xyz[j, 0], cb_xyz[j, 0]) + thresholds['ca_distances_calc'][1]\n",
        "        y_min_outer = min(ca_xyz[j, 1], cb_xyz[j, 1]) - thresholds['cb_distances_calc'][1]\n",
        "        y_max_outer = max(ca_xyz[j, 1], cb_xyz[j, 1]) + thresholds['cb_distances_calc'][1]\n",
        "        z_min_outer = min(ca_xyz[j, 2], cb_xyz[j, 2]) - thresholds['ca_distances_calc'][1]\n",
        "        z_max_outer = max(ca_xyz[j, 2], cb_xyz[j, 2]) + thresholds['ca_distances_calc'][1]\n",
        "        buffer = grid_resolution * 2\n",
        "        shared_x_min = max(shared_x_min, x_min_outer - buffer)\n",
        "        shared_x_max = min(shared_x_max, x_max_outer + buffer)\n",
        "        shared_y_min = max(shared_y_min, y_min_outer - buffer)\n",
        "        shared_y_max = min(shared_y_max, y_max_outer + buffer)\n",
        "        shared_z_min = max(shared_z_min, z_min_outer - buffer)\n",
        "        shared_z_max = min(shared_z_max, z_max_outer + buffer)\n",
        "\n",
        "    if shared_x_min >= shared_x_max or shared_y_min >= shared_y_max or shared_z_min >= shared_z_max:\n",
        "        return \"no metal\", 0, [None, None, None]\n",
        "\n",
        "    # --- Refined Grid Search (Find First Valid Candidate) ---\n",
        "    found_candidate_for_entry = False\n",
        "    candidate_coords = \"no metal\"\n",
        "    candidate_score = 0\n",
        "    candidate_angles = [None, None, None]\n",
        "\n",
        "    x_range = np.arange(shared_x_min, shared_x_max + 1e-9, grid_resolution)\n",
        "    y_range = np.arange(shared_y_min, shared_y_max + 1e-9, grid_resolution)\n",
        "    z_range = np.arange(shared_z_min, shared_z_max + 1e-9, grid_resolution)\n",
        "\n",
        "    if not (x_range.size > 0 and y_range.size > 0 and z_range.size > 0):\n",
        "         return \"no metal\", 0, [None, None, None]\n",
        "\n",
        "    # Grid search loops...\n",
        "    for x in x_range:\n",
        "        if found_candidate_for_entry: break\n",
        "        for y in y_range:\n",
        "            if found_candidate_for_entry: break\n",
        "            for z in z_range:\n",
        "                if found_candidate_for_entry: break\n",
        "                corner_point = np.array([x, y, z])\n",
        "                center_point = corner_point + grid_resolution / 2.0\n",
        "                points_to_check = [corner_point]\n",
        "                if np.all(center_point < [shared_x_max, shared_y_max, shared_z_max]):\n",
        "                    points_to_check.append(center_point)\n",
        "\n",
        "                for point in points_to_check:\n",
        "                    if found_candidate_for_entry: break\n",
        "                    # --- Filtering Cascade ---\n",
        "                    distances_ca = np.linalg.norm(ca_xyz - point, axis=1)\n",
        "                    distances_cb = np.linalg.norm(cb_xyz - point, axis=1)\n",
        "                    distance_ok = (np.all((thresholds['ca_distances_calc'][0] <= distances_ca) & (distances_ca <= thresholds['ca_distances_calc'][1])) and\n",
        "                                   np.all((thresholds['cb_distances_calc'][0] <= distances_cb) & (distances_cb <= thresholds['cb_distances_calc'][1])))\n",
        "                    if not distance_ok: continue\n",
        "\n",
        "                    angles = calculate_angles(point, ca_xyz, cb_xyz)\n",
        "                    angle_ok = all(thresholds['angle'][0] <= angle <= thresholds['angle'][1] for angle in angles)\n",
        "                    if not angle_ok: continue\n",
        "\n",
        "                    ratios = calculate_ratio(point, ca_xyz, cb_xyz)\n",
        "                    ratio_ok = np.all((thresholds['ratio'][0] <= ratios) & (ratios <= thresholds['ratio'][1]))\n",
        "                    if not ratio_ok: continue\n",
        "\n",
        "                    score = score_zn_predictions(distances_ca, distances_cb, angles, prob_map_3d, ca_bins, cb_bins, angle_bins)\n",
        "                    if score is None or score <= 0: continue\n",
        "\n",
        "                    # !!! Use the KDTree proximity filter !!!\n",
        "                    is_prox_valid = proximity_filter_kdtree(kdtree, point, exclusion_radius=2.5)\n",
        "                    if not is_prox_valid: continue\n",
        "\n",
        "                    # --- Candidate Found! ---\n",
        "                    candidate_coords = point\n",
        "                    candidate_score = score\n",
        "                    candidate_angles = angles\n",
        "                    found_candidate_for_entry = True\n",
        "                    break # Exit points_to_check loop\n",
        "\n",
        "    return candidate_coords, candidate_score, candidate_angles\n",
        "\n",
        "\n",
        "# --- Worker Function for Multiprocessing --- (No changes needed here)\n",
        "def process_pdb_entry(args):\n",
        "    # ... (This function remains the same as the previous multiprocessing version) ...\n",
        "    # ... (It unpacks args, loads structure, reads excel, loops through sites...) ...\n",
        "    # ... (Inside the loop, it calls the NEW estimate_zn_iterative) ...\n",
        "    # ... (It collects results, adds back to df, saves output) ...\n",
        "    # Unpack arguments\n",
        "    excel_path, Target_pdb_directory, base_name, Result_2_excel_directory, \\\n",
        "    thresholds_local, prob_map_3d_local, ca_bins_local, cb_bins_local, angle_bins_local = args\n",
        "\n",
        "    process_id = os.getpid()\n",
        "    print(f\"[PID {process_id}] Processing PDB ID: {base_name}\")\n",
        "\n",
        "    pdb_path = os.path.join(Target_pdb_directory, f\"{base_name}.pdb\")\n",
        "    if not os.path.exists(pdb_path):\n",
        "        print(f\"[PID {process_id}] ‚ùå Skipping {base_name}, PDB file not found at {pdb_path}.\")\n",
        "        return base_name, False, 0 # Return PDB ID, status, count\n",
        "\n",
        "    # Load structure ONCE for this PDB\n",
        "    structure = load_pdb_structure(base_name, Target_pdb_directory)\n",
        "    if structure is None:\n",
        "        print(f\"[PID {process_id}] ‚ùå Failed to load structure for {base_name}, skipping.\")\n",
        "        return base_name, False, 0\n",
        "\n",
        "    try:\n",
        "        df_alanine = pd.read_excel(excel_path)\n",
        "        if df_alanine.empty:\n",
        "            print(f\"[PID {process_id}] ‚ö†Ô∏è Input Excel file is empty for {base_name}, skipping.\")\n",
        "            return base_name, False, 0\n",
        "\n",
        "        df_alanine['PDB_ID'] = base_name\n",
        "        df_alanine = df_alanine.reset_index(drop=True)\n",
        "\n",
        "        # Check required columns...\n",
        "        ca_cols = ['CA1_X', 'CA1_Y', 'CA1_Z', 'CA2_X', 'CA2_Y', 'CA2_Z', 'CA3_X', 'CA3_Y', 'CA3_Z']\n",
        "        cb_cols = ['CB1_X', 'CB1_Y', 'CB1_Z', 'CB2_X', 'CB2_Y', 'CB2_Z', 'CB3_X', 'CB3_Y', 'CB3_Z']\n",
        "        res_num_cols = ['Coord_residue_number1', 'Coord_residue_number2', 'Coord_residue_number3']\n",
        "        required_input_cols = ca_cols + cb_cols + res_num_cols\n",
        "        if not all(col in df_alanine.columns for col in required_input_cols):\n",
        "            missing_cols = [col for col in required_input_cols if col not in df_alanine.columns]\n",
        "            print(f\"[PID {process_id}] ‚ùå Missing required columns in {os.path.basename(excel_path)}: {missing_cols}, skipping {base_name}.\")\n",
        "            return base_name, False, 0\n",
        "\n",
        "        # Initialize result columns\n",
        "        df_alanine['Zn_X_Grid'] = None\n",
        "        df_alanine['Zn_Y_Grid'] = None\n",
        "        df_alanine['Zn_Z_Grid'] = None\n",
        "        df_alanine['Zn_Score'] = 0.0\n",
        "        df_alanine['Angle_1'] = None\n",
        "        df_alanine['Angle_2'] = None\n",
        "        df_alanine['Angle_3'] = None\n",
        "\n",
        "        # Iterate through each site (row) in the dataframe for this PDB\n",
        "        for index, site_info in df_alanine.iterrows():\n",
        "            # Extract coordinates for this specific site (flat arrays)\n",
        "            ca_coords_site_flat = site_info[ca_cols].values\n",
        "            cb_coords_site_flat = site_info[cb_cols].values\n",
        "\n",
        "            # Call the modified estimate_zn_iterative for this site\n",
        "            zn_coords, zn_score, zn_angles = estimate_zn_iterative(\n",
        "                ca_coords_site_flat, cb_coords_site_flat, site_info, structure, # Pass site info and loaded structure\n",
        "                thresholds_local, prob_map_3d_local, ca_bins_local, cb_bins_local, angle_bins_local, # Pass data\n",
        "                grid_resolution=0.2\n",
        "            )\n",
        "\n",
        "            # Store results directly back into the DataFrame for this index\n",
        "            df_alanine.loc[index, 'Zn_Score'] = zn_score\n",
        "            if isinstance(zn_coords, np.ndarray):\n",
        "                df_alanine.loc[index, 'Zn_X_Grid'] = zn_coords[0]\n",
        "                df_alanine.loc[index, 'Zn_Y_Grid'] = zn_coords[1]\n",
        "                df_alanine.loc[index, 'Zn_Z_Grid'] = zn_coords[2]\n",
        "            if isinstance(zn_angles, (list, np.ndarray)) and len(zn_angles) == 3:\n",
        "                 df_alanine.loc[index, 'Angle_1'] = zn_angles[0]\n",
        "                 df_alanine.loc[index, 'Angle_2'] = zn_angles[1]\n",
        "                 df_alanine.loc[index, 'Angle_3'] = zn_angles[2]\n",
        "\n",
        "        # --- Post-processing and Saving Results ---\n",
        "        # Filter results after processing all sites for this PDB\n",
        "        df_output = df_alanine[df_alanine['Zn_Score'] > 0].copy()\n",
        "\n",
        "        if not df_output.empty:\n",
        "            output_file_path = os.path.join(Result_2_excel_directory, f\"{base_name}_result.xlsx\")\n",
        "            df_output.to_excel(output_file_path, index=False)\n",
        "            print(f\"[PID {process_id}] ‚úÖ Saved results for {len(df_output)} site(s) from {base_name} to: {os.path.basename(output_file_path)}\")\n",
        "            return base_name, True, len(df_output) # Return PDB ID, status, count\n",
        "        else:\n",
        "            print(f\"[PID {process_id}] ‚ö†Ô∏è No valid Zn predictions passed filters for {base_name}.\")\n",
        "            return base_name, True, 0 # Return PDB ID, status, count\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[PID {process_id}] ‚ùå Error processing {base_name}: {e}\")\n",
        "        print(traceback.format_exc())\n",
        "        return base_name, False, 0 # Return PDB ID, status, count\n",
        "\n",
        "\n",
        "# --- Main Execution Guard --- (No changes needed here)\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n--- Starting Main Process ---\")\n",
        "    # Data is loaded once here: thresholds, prob_map_3d, ca_bins, cb_bins, angle_bins\n",
        "\n",
        "    # Find input coordinate files using glob\n",
        "    print(f\"\\nSearching for input Excel files in: {Coordinate_excel_directory}\")\n",
        "    excel_files = glob.glob(os.path.join(Coordinate_excel_directory, '*_with_coordinates.xlsx'))\n",
        "    print(f\"Found {len(excel_files)} potential input files.\")\n",
        "\n",
        "    if not excel_files:\n",
        "        print(\"‚ùå No input Excel files found matching pattern '*_with_coordinates.xlsx'. Exiting.\")\n",
        "        exit()\n",
        "\n",
        "    # Prepare list of arguments for worker processes\n",
        "    tasks = []\n",
        "    for excel_path in excel_files:\n",
        "        base_name = os.path.basename(excel_path).replace('_with_coordinates.xlsx', '')\n",
        "        base_name = base_name.replace('processed_', '')\n",
        "        tasks.append((\n",
        "            excel_path, Target_pdb_directory, base_name, Result_2_excel_directory,\n",
        "            thresholds, prob_map_3d, ca_bins, cb_bins, angle_bins\n",
        "        ))\n",
        "\n",
        "    # Determine number of processes\n",
        "    num_processes = 8 # Example: Manually set if needed\n",
        "    print(f\"\\nInitializing multiprocessing pool with {num_processes} workers...\")\n",
        "\n",
        "    successful_files = 0\n",
        "    failed_files = 0\n",
        "    total_sites_saved = 0\n",
        "\n",
        "    # Create and run the pool\n",
        "    with multiprocessing.Pool(processes=num_processes) as pool:\n",
        "        print(\"Starting parallel processing...\")\n",
        "        results_iterator = pool.imap_unordered(process_pdb_entry, tasks)\n",
        "        processed_count = 0\n",
        "        for result in results_iterator:\n",
        "            processed_count += 1\n",
        "            pdb_id, status, site_count = result\n",
        "            if status:\n",
        "                successful_files += 1\n",
        "                total_sites_saved += site_count\n",
        "                print(f\"  ({processed_count}/{len(tasks)}) Completed: {pdb_id} ({site_count} sites saved)\")\n",
        "            else:\n",
        "                failed_files += 1\n",
        "                print(f\"  ({processed_count}/{len(tasks)}) Failed/Skipped: {pdb_id}\")\n",
        "\n",
        "    print(\"\\n--- Multiprocessing Pool Finished ---\")\n",
        "    print(f\"\\nüèÅ Batch processing summary:\")\n",
        "    print(f\"  Total input files found: {len(tasks)}\")\n",
        "    print(f\"  Successfully processed files: {successful_files}\")\n",
        "    print(f\"  Failed/Skipped files: {failed_files}\")\n",
        "    print(f\"  Total result sites saved: {total_sites_saved}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "HHlEaC3XuxM8"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from Bio.PDB import PDBParser\n",
        "import requests\n",
        "\n",
        "# Markdown documentation for file pathways\n",
        "\n",
        "# @markdown # Step 5: Analyze output\n",
        "\n",
        "# Load input file\n",
        "input_file_path = \"/content/result/\" # @param {type:\"string\"}\n",
        "df_new = pd.read_excel(input_file_path)\n",
        "\n",
        "# Generate PyMOL script file\n",
        "pymol_script_commands = []\n",
        "df_new['Combination_Number'] = range(1, len(df_new) + 1)\n",
        "\n",
        "# Generate the PyMOL script for both valid and invalid Zn binding forms\n",
        "for index, row in df_new.iterrows():\n",
        "    # Retrieve chain and residue information\n",
        "    chain1, res1 = row['Coord_chain_id_number1'], row['Coord_residue_number1']\n",
        "    chain2, res2 = row['Coord_chain_id_number2'], row['Coord_residue_number2']\n",
        "    chain3, res3 = row['Coord_chain_id_number3'], row['Coord_residue_number3']\n",
        "\n",
        "    # Retrieve Zn coordinates\n",
        "    zn_x, zn_y, zn_z = row['Zn_X_Grid'], row['Zn_Y_Grid'], row['Zn_Z_Grid']\n",
        "\n",
        "    selection_name = f\"obj{row['Combination_Number']:02d}\"\n",
        "\n",
        "    # Select the residues\n",
        "    pymol_script_commands.append(f\"select {selection_name}, (chain {chain1} and resi {res1}) or (chain {chain2} and resi {res2}) or (chain {chain3} and resi {res3})\")\n",
        "\n",
        "    # Create the objects for the residues\n",
        "    pymol_script_commands.append(f\"create {selection_name}_residue1, /{row['PDB_ID']}//{chain1}/{res1}\")\n",
        "    pymol_script_commands.append(f\"create {selection_name}_residue2, /{row['PDB_ID']}//{chain2}/{res2}\")\n",
        "    pymol_script_commands.append(f\"create {selection_name}_residue3, /{row['PDB_ID']}//{chain3}/{res3}\")\n",
        "\n",
        "    # Check if Zn coordinates are available\n",
        "    if not pd.isna(zn_x) and not pd.isna(zn_y) and not pd.isna(zn_z):\n",
        "        # Zn coordinates are present, add the Zn pseudoatom\n",
        "        zn_name = f\"{selection_name}_Metal\"\n",
        "        pymol_script_commands.append(f\"pseudoatom {zn_name}, pos=[{zn_x}, {zn_y}, {zn_z}], elem=Metal, name={zn_name}\")\n",
        "        pymol_script_commands.append(f\"show sphere, {zn_name}\")\n",
        "    else:\n",
        "        # Zn coordinates are missing, mark this combination as non-binding\n",
        "        pymol_script_commands.append(f\"# {selection_name} does not bind Zn\")\n",
        "\n",
        "# Save the commands into a PyMOL script\n",
        "pymol_script_file = \"/content/1ips_final.pml\" # @param {type:\"string\"}\n",
        "with open(pymol_script_file, 'w') as f:\n",
        "    f.write(\"# PyMOL script for visualizing both Zn-binding and non-binding residue combinations\\n\\n\")\n",
        "    for command in pymol_script_commands:\n",
        "        f.write(command + '\\n')\n",
        "\n",
        "print(f\"PyMOL script saved to {pymol_script_file}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}